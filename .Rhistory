yields <- c(sand, clay, loam)
dati.new <- data.frame(soil.type, yields)
dati.new
by(dati.new$yields, dati.new$soil.type, var)
mod <- lm(yields ~ soil.type, data=dati.new)
summary(mod)
# Tukey Honestly Significant Differences
TukeyHSD(anova.yields) # where anova.yields comes from aov()
anova.yields <- aov(yields ~ soil.type, data=dati.new)
plot(anova.yields)
sand <- c(6,10,8,6,14,17,9,11,7,11)
clay <- c(17,15,3,11,14,12,12,8,10,13)
loam <- c(13,16,9,12,15,16,17,13,18,14)
dati <- data.frame(sand,clay,loam)
dati
colMeans(dati)
boxplot(dati)
soil.type <- c(rep("sand",10),
rep("clay",10),
rep("loam",10))
yields <- c(sand, clay, loam)
dati.new <- data.frame(soil.type, yields)
dati.new
by(dati.new$yields, dati.new$soil.type, var)
sand <- c(6,10,8,6,14,17,9,11,7,11)
clay <- c(17,15,3,11,14,12,12,8,10,13)
loam <- c(13,16,9,12,15,16,17,13,18,14)
dati <- data.frame(sand,clay,loam)
dati
colMeans(dati)
boxplot(dati)
soil.type <- c(rep("sand",10),
rep("clay",10),
rep("loam",10))
yields <- c(sand, clay, loam)
dati.new <- data.frame(soil.type, yields)
dati.new
by(dati.new$yields, dati.new$soil.type, var)
anova.yields <- aov(yields ~ soil.type, data=dati.new)
summary(anova.yields)
# Tukey Honestly Significant Differences
TukeyHSD(anova.yields) # where anova.yields comes from aov()
plot(TukeyHSD(anova.yields))
pairwise.t.test(dati.new$yields, dati.new$soil.type, p.adjust.method = "bonferroni")
library(MASS)
data(cats)
head(cats)
str(cats)
plot(cats$Bwt, cats$Hwt)
plot(cats$Hwt ~ cats$Bwt)
plot(Hwt ~ Bwt, data = cats)
plot(Bwt ~ Hwt, data = cats)
cor(cats$Bwt, cats$Hwt)
cor.test(cats$Bwt, cats$Hwt)
cor.test(cats$Hwt, cats$Bwt)
mod <- lm(Hwt ~ Bwt, data=cats)
summary(mod)
plot(cats$Bwt, cats$Hwt)
abline(coef(mod), col="red")
par(mfrow=c(2,2))
plot(mod)
par(mfrow=c(1,4))
plot(mod)
par(mfrow=c(1,1))
plot(mod)
library(car)
library(car)
library(car)
library(car)
library(dplyr)
library(ggcorrplot)
# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
# set.seed(123)
# training.samples <- Boston$medv %>%
#   createDataPartition(p = 0.8, list = FALSE)
# train.data  <- Boston[training.samples, ]
# test.data <- Boston[-training.samples, ]
head(Boston)
pred <- dplyr::select(Boston, crim:lstat)
names(pred)
pred
cor.matrix <- cor(pred)
cor.matrix
head(Boston)
ggcorrplot(cor.matrix)
ggcorrplot(cor(pred))
ggcorrplot(cor.matrix, method = "circle")
ggcorrplot(cor.matrix, method = "circle", lab=TRUE)
ggcorrplot(cor.matrix, lab=TRUE)
ggcorrplot(cor.matrix, method = "circle", lab=TRUE)
p.mat <- (abs(cor.matrix) > 0.7)
p.mat
p.mat <- (abs(cor.matrix) > 0.7)
p.mat
colSums(p.mat)
colSums(p.mat)-1
p.mat <- (abs(cor.matrix) > 0.7)
p.mat
colSums(p.mat)-1 #quante correlazioni sopra 0.7 o sotto -0.7
p.mat <- (abs(cor.matrix) > 0.7)
p.mat
br()
p.mat <- (abs(cor.matrix) > 0.7)
p.mat
colSums(p.mat)-1 #quante correlazioni sopra 0.7 o sotto -0.7
# Argument p.mat
# Barring the significant coefficients
ggcorrplot(cor.matrix,
hc.order = TRUE, method = "circle",
type = "higher", p.mat = p.mat
)
# Argument p.mat
# Barring the significant coefficients
ggcorrplot(cor.matrix,
hc.order = TRUE, method = "circle",
type = "upper", p.mat = p.mat
)
# Argument p.mat
# Barring the significant coefficients
ggcorrplot(cor.matrix,
hc.order = TRUE, method = "circle",
type = "lower", p.mat = p.mat
)
# Build the model
model1 <- lm(medv ~ ., data = Boston)
summary(model1)
vif(model1)
pollution <- read.table("data/sulphur.dioxide.txt", header=TRUE)
View(pollution)
summary(pollution)
pollution <- read.table("data/sulphur.dioxide.txt", header=TRUE)
summary(pollution)
view(pollution)
pollution <- read.table("data/sulphur.dioxide.txt", header=TRUE)
summary(pollution)
View(pollution)
pairs(pollution,panel=panel.smooth)
cor.matrix <- cor(pollution)
# cor.matrix
p.mat <- (abs(cor.matrix) > 0.7)
# p.mat
ggcorrplot(cor.matrix,
hc.order = TRUE, method = "circle",
type = "lower", p.mat = p.mat
)
# Build the model
model1 <- lm(Pollution ~ ., data = pollution)
summary(model1)
vif(model1)
model1 <- lm(Pollution ~ Temp + Industry + Wind + Rain + Wet.days, data = pollution)
summary(model1)
dati.mod <- read.csv("data/dati.mod.csv")
head(dati.mod)
varexpl <- dati.mod[,8:15]
head(varexpl)
hist(dati.mod$CattSiNo)
table(dati.mod$CattSiNo)
summary(varexpl)
with(dati.mod,
boxplot(Incl ~ CattSiNo, main="Slope"))
with(dati.mod,
boxplot(Alt ~ CattSiNo, main="Altitude"))
with(dati.mod,
boxplot(copveg ~ CattSiNo, main="Veg cover"))
with(dati.mod,
boxplot(NSpecie ~ CattSiNo, main="N species"))
with(dati.mod,
boxplot(media.rocce ~ CattSiNo, main="Mean diam"))
with(dati.mod,
boxplot(mediana.rocce ~ CattSiNo, main="Median diam"))
with(dati.mod,
boxplot(sd.rocce ~ CattSiNo, main="SD diam"))
with(dati.mod,
boxplot(Esp.tr ~ CattSiNo, main="Aspect"))
par(mfrow=c(1,2))
dotchart(dati.mod$sd.rocce,
ylab = "Order of observations",
xlab = "SD diam", main = "")
dotchart(dati.mod$sd.rocce,
groups = dati.mod$CattSiNo,
ylab = "CattSiNo",
xlab = "SD diam", main = "")
par(mfrow=c(1,1))
# eliminazione dell'outlier
subset(dati.mod, dati.mod$sd.rocce > 200)
dati.mod.new <- subset(dati.mod, dati.mod$sd.rocce < 200)
# trasformazione della variabile x
boxplot(dati.mod$sd.rocce)
boxplot(log(dati.mod$sd.rocce))
dati.mod.new2 <- cbind(dati.mod, logSD = log(dati.mod$sd.rocce))
dati.mod.new2 <- data.frame(dati.mod, logSD = log(dati.mod$sd.rocce))
head(dati.mod.new2)
cor(varexpl)
fm1 <- glm(CattSiNo ~ Incl +  # BEYOND OPTIMAL MODEL
Alt +
copveg +
# NSpecie +
# media.rocce +
mediana.rocce +
sd.rocce +
Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1)
hist(dati.mod.new$Ncatture)
fm1.pois <- glm(Ncatture ~ Incl +
Alt +
copveg +
# NSpecie +
# media.rocce +
mediana.rocce +
sd.rocce +
Esp.tr,
family=poisson, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1.pois)
# UP TO YOU!!!
fm1.noexp <- glm(CattSiNo ~ Incl +
Alt,
#copveg,
# NSpecie +
# media.rocce +
#mediana.rocce,
# sd.rocce, #
# Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1.noexp)
# UP TO YOU!!!
fm1.noexp <- glm(CattSiNo ~ Incl +
Alt,
#copveg +
# NSpecie +
# media.rocce +
#mediana.rocce,
# sd.rocce, #
# Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1.noexp)
fm2 <- glm(CattSiNo ~ #Incl +
Alt +
# copveg +
# NSpecie +
media.rocce, #+
# mediana.rocce +
# sd.rocce +
# Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
par(mfrow=c(1,2))
termplot(fm2, se=TRUE, col.term = "red", col.se = "grey",
ylabs = rep("odds ratio",2),
main = c("Alt","Mean diam"), cex.main = 0.7)
par(mfrow=c(1,1))
logit2prob <- function(logit){
odds <- exp(logit)
prob <- odds / (1 + odds)
return(prob)
}
predict.alt <- predict(fm2,
data.frame(Alt=dati.mod.new$Alt,
media.rocce=mean(dati.mod.new$media.rocce)),
type = "response")
tab <- data.frame(Alt=dati.mod.new$Alt,
prob = predict.alt)
# tab
library(dplyr)
arrange(tab, Alt)
plot(arrange(tab, Alt))
library(sjPlot)
plot_model(fm2, type = "eff", terms = c("Alt"))
plot_model(fm2)
fm2 <- glm(CattSiNo ~ Alt + I(Alt^2),
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
# ok:
fm2 <- glm(CattSiNo ~ Alt,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
# no:
fm2 <- glm(CattSiNo ~ I(Alt^2),
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
knitr::opts_chunk$set(echo = TRUE)
res.prcomp <- prcomp(iris[,-5])
summary(res.prcomp)
# compare to:
res.prcomp <- prcomp(iris[,-5], center=TRUE, scale=TRUE)
summary(res.prcomp)
plot(res.prcomp)
biplot(res.prcomp)
res.prcomp$rotation
biplot(res.prcomp, choices=3:4)
res.princomp <- princomp(iris[,-5])
summary(res.princomp)
# compare to:
res.princomp <- princomp(iris[,-5], cor=TRUE)
summary(res.princomp)
loadings(res.princomp) # Small loadings are conventionally not printed (replaced by spaces), to draw the eye to the pattern of the larger loadings.
# A matrix of loadings, one column for each factor. The factors are ordered in decreasing order of sums of squares of loadings, and given the sign that will make the sum of the loadings positive.
# The signs of the loadings vectors are arbitrary
# Calling loadings() on your object returns a summary where the SS are always equal to 1, hence the % variance is just the SS loadings divided by the number of variables. It makes sense only when using Factor Analysis - don't look at this table for PCA!!
plot(res.princomp)
biplot(res.princomp)
biplot(res.prcomp)
biplot(res.prcomp)
res.prcomp$rotation
res.prcomp <- prcomp(iris[,-5])
summary(res.prcomp)
# compare to:
res.prcomp <- prcomp(iris[,-5], center=TRUE, scale=TRUE)
summary(res.prcomp)
plot(res.prcomp)
biplot(res.prcomp)
res.prcomp$rotation
biplot(res.prcomp, choices=3:4)
res.princomp <- princomp(iris[,-5])
summary(res.princomp)
# compare to:
res.princomp <- princomp(iris[,-5], cor=TRUE)
summary(res.princomp)
loadings(res.princomp) # Small loadings are conventionally not printed (replaced by spaces), to draw the eye to the pattern of the larger loadings.
# A matrix of loadings, one column for each factor. The factors are ordered in decreasing order of sums of squares of loadings, and given the sign that will make the sum of the loadings positive.
# The signs of the loadings vectors are arbitrary
# Calling loadings() on your object returns a summary where the SS are always equal to 1, hence the % variance is just the SS loadings divided by the number of variables. It makes sense only when using Factor Analysis - don't look at this table for PCA!!
plot(res.princomp)
biplot(res.princomp)
library(FactoMineR)
library(factoextra)
res.pca <- PCA(iris[, -5],  graph = FALSE)
res.pca
get_eig(res.pca) # Extract eigenvalues/variances
sqrt(get_eig(res.pca)[,1]) # compare to summary(res.prcomp)
# Visualize eigenvalues/variances
fviz_eig(res.pca, addlabels=TRUE, hjust = -0.3)+
theme_minimal()
# export all results in csv file
write.infile(res.pca,file="my_FactoMineR_results.csv")
varimax(res.princomp$loadings[,1:2])
# Varimax is so called because it maximizes the sum of the variances of the squared loadings (squared correlations between variables and factors).
# After an orthogonal rotation (such as varimax), the "rotated-principal" axes are not orthogonal, and orthogonal projections on them do not make sense. So one should rather drop this whole axes/projections point of view. It would be weird to still call it PCA (which is all about projections with maximal variance etc.).
# Extract the results for variables
var <- get_pca_var(res.pca)
var
# Contribution of variables
var$contrib
colSums(var$contrib)
# correlation of variables with the first 4 PC
res.pca$var$cor[, 1:4]
# axis characterization
dimdesc(res.pca)
# Graph of variables: default plot
fviz_pca_var(res.pca, col.var = "steelblue")
# It’s possible to control variable colors using their contributions to the principal axes:
# Control variable colors using their contributions
# Use gradient color
fviz_pca_var(res.pca, col.var="contrib")+
scale_color_gradient2(low="white", mid="blue",
high="red", midpoint = 25) +
theme_minimal()
# Variable contributions on axis 1
fviz_contrib(res.pca, choice="var", axes = 1 )+
labs(title = "Contributions to Dim 1")
# Variable contributions on axis 2
fviz_contrib(res.pca, choice="var", axes = 2 )+
labs(title = "Contributions to Dim 2")
# Variable contributions on axes 1 + 2
fviz_contrib(res.pca, choice="var", axes = 1:2)+
labs(title = "Contributions to Dim 1+2")
# Extract the results for individuals
ind <- get_pca_ind(res.pca)
ind
# Coordinates of individuals
head(ind$coord)
# compare to:
head(res.prcomp$x)
# Graph of individuals
# 1. Use repel = TRUE to avoid overplotting
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
print(p)
# Change group colors manually
# Read more: http://www.sthda.com/english/wiki/ggplot2-colors
p + scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
theme_minimal()
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Extract the results for individuals
ind <- get_pca_ind(res.pca)
ind
# Coordinates of individuals
head(ind$coord)
# compare to:
head(res.prcomp$x)
# Graph of individuals
# 1. Use repel = TRUE to avoid overplotting
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
print(p)
# Change group colors manually
# Read more: http://www.sthda.com/english/wiki/ggplot2-colors
p + scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
theme_minimal()
colorvar<-col="red"
colorvar->col="red"
colorvar<-col="red"
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
# Extract the results for individuals
ind <- get_pca_ind(res.pca)
ind
# Coordinates of individuals
head(ind$coord)
# compare to:
head(res.prcomp$x)
# Graph of individuals
# 1. Use repel = TRUE to avoid overplotting
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
print(p)
# Change group colors manually
# Read more: http://www.sthda.com/english/wiki/ggplot2-colors
p + scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
theme_minimal()
# Biplot of individuals and variables
# ++++++++++++++++++++++++++
# Only variables are labelled
fviz_pca_biplot(res.pca,  label="var", habillage=iris$Species,
addEllipses=TRUE, ellipse.level=0.95) +
theme_minimal()
# # df <- USArrests[,1:2]
# df_birds <- read_xlsx("data/short.xlsx", col_types = c("text","numeric","numeric"))
# df <- na.omit(df_birds)
# df <- scale(df[,2:3])
# head(df)
european_birds <- read_delim("data/european_birds.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
fviz_nbclust(df, kmeans, method = "wss")
---
title: "K-means clustering - step by step"
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(FactoMineR)
library(readxl)
library(readr)
# # df <- USArrests[,1:2]
# df_birds <- read_xlsx("data/short.xlsx", col_types = c("text","numeric","numeric"))
# df <- na.omit(df_birds)
# df <- scale(df[,2:3])
# head(df)
european_birds <- read_delim("data/european_birds.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
european_birds %>%
dplyr::select(Species,
LengthU_MEAN,
# WingU_MEAN,
Clutch_MEAN,
`Life span`) -> df_birds
df_birds <- na.omit(df_birds)
df <- scale(df_birds[,2:dim(df_birds)[2]])
head(df)
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
fviz_nbclust(df, kmeans, method = "wss")
fviz_nbclust(df, kmeans, method = "silhouette")
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 3, nstart = 25)
print(final)
fviz_cluster(final, data = df,
geom = c("point"),
ellipse.type = "euclid")
final$cluster
df_birds %>%
mutate(cluster_id = final$cluster) -> df_birds_cluster
write.csv(df_birds_cluster, file = "output/df_birds_cluster.csv")
writexl::write_xlsx(df_birds_cluster, "output/df_birds_cluster.xlsx")
head(df_birds_cluster)
