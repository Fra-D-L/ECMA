barplot(table(roll1 + roll2), main = "2 Dice Sum, 100 Rolls")  #this works better for this case
rolls <- roll1 + roll2
sum(rolls == 7)
sum(rolls == 7)/n
n = 1500
roll1 = NULL  #This initializes our variable - i.e. it creates a spot in memory for it. We need to do this for any vector, table, matrix, dataframe, but not for single numbers
roll2 = NULL
for (i in 1:n) {
roll1[i] = RollDie(1)
roll2[i] = RollDie(1)
}
# We can ask how many times this happen:
sum(roll1 == 6 & roll2 == 6)
# the relative frequency is:
sum(roll1 == 6 & roll2 == 6)/n
(1/6)*(1/6)
0.02777778+0.02777778+0.02777778+0.02777778+0.02777778+0.02777778
# or
0.02777778*6
y <- read.csv("data/captures.csv",sep=";")
y <- na.omit(y[,c("age", "sex")])
y <- read.csv("data/captures.csv",sep=";")
y <- na.omit(y[,c("age", "sex")])
y <- read.csv("data/captures.csv",sep=";")
y <- na.omit(y[,c("age", "sex")]); y
library(dplyr)
age.marginal.df <-
y %>%
group_by(age) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
age.marginal.df
library(dplyr)
age.marginal.df <-
y %>%
group_by(age) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
age.marginal.df
library(dplyr)
age.marginal.df <-
y %>%
group_by(age) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
age.marginal.df
mutate(ungroup(summarise(n = n(group_by(y, age)))), prop = n/sum(n))
library(dplyr)
age.marginal.df <-
y %>%
group_by(age) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
age.marginal.df
mutate(ungroup(summarise(n = n(group_by(y, age)))), prop = n/sum(n))
library(dplyr)
age.marginal.df <-
y %>%
group_by(age) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
age.marginal.df
sex.marginal.df <-
y %>%
group_by(sex) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
sex.marginal.df
joint.df <-
y %>%
group_by(age, sex) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
joint.df
joint.prob <-
joint.df %>%
filter(age == "A", sex == "F") %>%
.$prop
joint.prob
joint.prob <-
joint.df %>%
filter(age == "G", sex == "F") %>%
.$prop
joint.prob
joint.prob <-
joint.df %>%
filter(age == "A", sex == "F") %>%
.$prop
joint.prob
marg.prob <-
age.marginal.df %>%
filter(age == "A") %>%
.$prop
marg.prob
marg.prob <-
age.marginal.df %>%
filter(age == "A") %>%
.$prop
marg.prob
marg.prob <-
age.marginal.df %>%
filter(age == "A") %>%
.$prop
marg.prob
marg.prob <-
age.marginal.df %>%
filter(age == "A") %>%
.$prop
marg.prob
marg.prob <-
age.marginal.df %>%
filter(age == "A") %>%
.$prop
marg.prob
library(Rmisc)
library(Rmisc)
library(Hmisc)
library(ggplot2)
library(boot)
dati <- read.csv("data/captures.csv", sep=";")
CI(dati$weight_g, ci=0.95)
CI(na.omit(dati$weight_g), ci=0.95)
dati2 <- na.omit(dati)
group.CI(weight_g ~ sex,
data= dati2,
ci=0.95)
intervalli <- group.CI(weight_g ~ sex,
data= dati2,
ci=0.95)
intervalli
ggplot(intervalli, aes(x=sex, y=weight_g.mean, colour=sex)) +
geom_errorbar(aes(ymin=weight_g.lower, ymax=weight_g.upper), width=.1) +
geom_line() +
geom_point()
as.data.frame(CI(na.omit(dati$weight_g), ci=0.95)) -> CImean
CImean
variabile <- dati2$footlength_mm # o qualsiasi altra variabile numerica
# script 'generalizzato'
intervalli <- group.CI(variabile ~ sex,
data= dati2,
ci=0.95)
intervalli
ggplot(intervalli, aes(x=sex, y=variabile.mean, colour=sex)) +
geom_errorbar(aes(ymin=variabile.lower, ymax=variabile.upper), width=.1) +
geom_line() +
geom_point()
boot.data <- boot(dati2$weight_g,
function(x,i) mean(x[i]),
R=10000)
boot.ci(boot.data,
conf = 0.95)
mean(dati2$weight_g)
library(tidyverse)
dati2 %>%
filter(sex == "F") -> datiF
dati2 %>%
filter(sex == "M") -> datiM
dataset <- datiM
boot.data <- boot(dataset$weight_g,
function(x,i) mean(x[i]),
R=10000)
boot.ci(boot.data,
conf = 0.95)
t.test(y$weight_g)
y <- captures
library(tidyverse)
dati2 %>%
filter(sex == "F") -> datiF
dati2 %>%
filter(sex == "M") -> datiM
dataset <- datiM
boot.data <- boot(dataset$weight_g,
function(x,i) mean(x[i]),
R=10000)
library(boot)
library(boot)
library(tidyverse)
dati2 %>%
filter(sex == "F") -> datiF
dati2 %>%
filter(sex == "M") -> datiM
dataset <- datiM
boot.data <- boot(dataset$weight_g,
function(x,i) mean(x[i]),
R=10000)
boot.ci(boot.data,
conf = 0.95)
library(dplyr)
captures <- read.csv("data/captures.csv", sep=";")
captures %>%
filter(age=="A") -> adults
head(adults)
adults %>%
group_by(animal_id, sex) -> grouped
grouped %>%
arrange(animal_id)
grouped %>%
summarise(count=n(), individual.weight = mean(weight_g, na.rm = TRUE)) -> ind.w
ind.w
# use this code only if you could not install the dplyr package
captures <- read.csv("data/captures.csv", sep=";")
adults <- subset(captures, age=="A")
adults
aa <- tapply(adults$weight_g, adults$animal_id, mean, na.rm = TRUE)
bb <- data.frame(aa, row.names(aa))
colnames(bb) <- c("individual.weight","animal_id")
bb
cc <- table(adults$sex, adults$animal_id)
dd <- as.data.frame(cc)
ee <- subset(dd, Freq > 0)
colnames(ee) <- c("sex","animal_id","count")
ee
ind.w <- merge(bb, ee, by="animal_id")
#?qqplot
qqnorm(ind.w$individual.weight); qqline(ind.w$individual.weight, col="red")
shapiro.test(ind.w$individual.weight)
t.test(ind.w$individual.weight, mu = 30)
length(na.omit(ind.w$individual.weight))
length(na.omit(ind.w$individual.weight))
length(na.omit(ind.w$individual.weight))-1
## is the weight of females and males different?
# we use again the summarise dplyr function
ind.w %>%
group_by(sex) %>%
summarise(count=n(), class.weight = mean(individual.weight, na.rm = TRUE))
# again, if you could not install dplyr, use the following code
# tapply(ind.w$individual.weight, ind.w$sex, mean, na.rm = TRUE)
# We can see the difference in weight that we observe between males and females
w.f <- (subset(ind.w, sex=="F"))$individual.weight
w.m <- (subset(ind.w, sex=="M"))$individual.weight
w.f
w.m
par(mfrow=c(1,2))
qqnorm(w.f); qqline(w.f, col="red")
qqnorm(w.m); qqline(w.m, col="red")
shapiro.test(w.f)
shapiro.test(w.m)
x <- rnorm(100,0,1)
curve(df(x, df1=1, df2=1), from=0, to=5, lty=1, ylim=c(0,2))
curve(df(x, df1=2, df2=1), from=0, to=5, lty=2, add=T)
curve(df(x, df1=5, df2=2), from=0, to=5, lty=3, add=T)
curve(df(x, df1=100, df2=1), from=0, to=5, lty=4, add=T)
curve(df(x, df1=100, df2=100), from=0, to=5, lty=5, add=T)
var(w.f, na.rm = T)
var(w.m, na.rm = T)
var.test(w.f,w.m)
var(w.f, na.rm = T)/var(w.m, na.rm = T)
boxplot(ind.w$individual.weight ~ ind.w$sex)
(0.86+1.96)/sqrt(24)
(0.86-1.96)/sqrt(24)
0.86+(1.96)/sqrt(24)
5*6
c<-5
c
# ?t.test
t.test(w.f, w.m,
var.equal = TRUE,
paired = FALSE,
alternative = "two.sided") # greater/less
square <- formula(x^2)
square <- formula(x*x)
x <- c(15, 10, 13, 7, 9, 8, 21, 9, 14, 8)
y <- c(15, 14, 12, 8, 14, 7, 16, 10, 15, 12)
# normality test
# with a qqplot
par(mfrow=c(1,2))
qqnorm(x); qqline(x, col="red")
qqnorm(y); qqline(y, col="red")
# with the Shapiro-Wilk test
shapiro.test(x)
shapiro.test(y)
# normality test
# with a qqplot
par(mfrow=c(1,2))
qqnorm(x); qqline(x, col="red"); qqnorm(y); qqline(y, col="red")
# with the Shapiro-Wilk test
shapiro.test(x); shapiro.test(y)
# homogeneity of variances
var(x)/var(y)
var.test(x,y)
#t-test
t.test(x,y,
paired = FALSE,
var.equal = TRUE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = FALSE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = FALSE,
alternative = "two.sided")
x <- c(3, 0, 5, 2, 5, 5, 5, 4, 4, 5)
y <- c(2, 1, 4, 1, 4, 3, 3, 2, 3, 5)
# normality test
# with a qqplot
par(mfrow=c(1,2))
qqnorm(x); qqline(x, col="red")
qqnorm(y); qqline(y, col="red")
# with the Shapiro-Wilk test
shapiro.test(x)
shapiro.test(y)
# homogeneity of variances
var.test(x,y)
# t-test
#t-test
t.test(x,y,
paired = TRUE,
var.equal = TRUE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = TRUE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = TRUE,
alternative = "two.sided")
rep(5/2, 5)
library(dplyr)
## chi2 - goodness of fit test
freq <- c(22, 21, 22, 27, 22, 36)
probs <- rep(1/6, 6)
chisq.test(freq, p = probs)
## chi2 - test of independence
yesbelt <- c(12813, 647, 359, 42)
nobelt <- c(65963, 4000, 2642, 303)
chisq.test(data.frame(yesbelt, nobelt))
# names(chisq.test(data.frame(yesbelt, nobelt)))
# data.frame(yesbelt, nobelt)
# rowSums(data.frame(yesbelt, nobelt))
# colSums(data.frame(yesbelt, nobelt))
# # for the first cell
# 78776*13861/sum(data.frame(yesbelt, nobelt))
# # for the whole dataframe
# n = sum(data.frame(yesbelt, nobelt))
# data.frame(yesbelt, nobelt) %>%
#   mutate(somma.riga = rowSums(data.frame(yesbelt, nobelt)),
#          e.yesbelt = somma.riga*sum(yesbelt)/n,
#          e.nobelt = somma.riga*sum(nobelt)/n)
# # compare with the values calculated by R
# chisq.test(data.frame(yesbelt, nobelt))$exp
#
# chisq.test(data.frame(yesbelt, nobelt))
## chi2 - test of homogeneity
die.fair <- sample(1:6, 200, p=c(1,1,1,1,1,1)/6, replace = TRUE)
die.bias <- sample(1:6, 200, p=c(0.5,0.5,1,1,1,2)/6, replace = TRUE)
res.fair <- table(die.fair)
res.bias <- table(die.bias)
rbind(res.fair, res.bias)
chisq.test(rbind(res.fair, res.bias))
names(chisq.test(rbind(res.fair, res.bias)))
freq <- c(53, 22, 49)
probs <- c(5/12, 3/12, 4/12)
chisq.test(freq, p = probs)
r1 <- c(3,1)
r2 <- c(1,0)
chisq.test(data.frame(r1,r2)) -> Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
r1 <- c(3,1)
r2 <- c(1,0)
chisq.test(data.frame(r1,r2)) -> Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
r1 <- c(3,1)
r2 <- c(1,0)
chisq.test(data.frame(r1,r2)) -> Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
birds_habitats <- data.frame(
species = c("Ruby-crowed kinglet", "White-crowned sparrow", "Lincoln's sparrow",
"Golgen-crowded sparrow", "Bushtit", "Song sparrow", "Spotted towhee",
"Bewick's wren", "Hermit thrush", "Dard-eyed junco",
"Lesser goldfinch", "Uncommon"),
Remnant = c(677, 408, 270, 300, 198, 150, 137, 106, 119, 34, 57, 457),
Restored = c(198, 260, 187, 89, 91, 50, 32, 48, 24, 39, 15, 125)
)
birds_habitats
chisq.test(data.frame(birds_habitats$Remnant, birds_habitats$Restored))
# ?cars
summary(cars)
?cars
# ?cars
summary(cars)
# ?cars
summary(cars)
cars.lm <- lm(dist ~ speed, data = cars)
names(cars.lm)
coef(cars.lm)
cars.lm
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red", lwd=2)
summary(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red", lwd=2)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red", lwd=2)
summary(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
plot(dist ~ speed, data = cars, pch = 16)
abline(cars.lm, col="red", lwd=2)
plot(dist ~ speed, data = cars, pch = 16)
abline(cars.lm, col="red", lwd=2)
summary(cars.lm)
-17.58 + 3.93 * (8) # point estimate
cars[5,] # real data
fitted(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
predict(cars.lm, newdata = data.frame(speed = c(0,6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
col="blue", pch=19)
predict(cars.lm, newdata = data.frame(speed = c(0,6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
col="blue", pch=19)
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
predict(cars.lm, newdata = data.frame(speed = c(0,6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
col="blue", pch=19)
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
carslm$residuals-carsumry$sigma
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
carslm$residuals-carsumry$sigma
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
cars.lm$residuals-carsumry$sigma
cars.lm$residuals-carsumry$sigma
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
cars.lm$residuals-carsumry$sigma
summary(cars.lm)
confint(cars.lm)
# new <- data.frame(speed = c(5,6,21))
# predict(cars.lm, newdata = new, interval = "confidence")
# predict(cars.lm, newdata = new, interval = "prediction")
library(HH)
ci.plot(cars.lm)
# or
# library(UsingR)
# simple.lm(cars$speed, cars$dist, show.ci=TRUE)
x = c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
y = c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
lm(y ~ x) # the basic values of the regression analysis
lm.result = lm(y ~ x)
summary(lm.result)
plot(x,y, xlim=c(0,80), ylim=c(150,240)) # make a plot
abline(coef(lm.result)) #lm(y ~ x)) # plot the regression line
# to see only some parts of the result
coef(lm.result)
summary(resid(lm.result))
void main() {
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# SE.b0 = s*(sqrt((1/n)+(mean(x)^2/sum((x-mean(x))^2)))) # formula for the standard error of the intercept
SE.b0 = 2.86694
b0 = coef(lm.result)[[1]]
t = (b0 - 220)/SE.b0
t
pt(t, 13, lower.tail = TRUE) # FIND THE LEFT TAIL FOR THIS VALUE OF t AND 15-2 df
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# n = length(x)
# es = resid(lm.result)
# b1 = coef(lm.result)[[2]]
# s = sqrt(sum(es^2)/(n-2)) # S^2 = SSE/(n-2), s is the residual standard error
# SE = s/sqrt(sum((x-mean(x))^2)) # SE
b1 = -0.79773
SE = 0.06996
t = (b1-(-1))/SE # (obs. value - hyp.value)/SE
t
pt(t, 13, lower.tail = FALSE) # FIND THE RIGHT TAIL FOR THIS VALUE OF t AND 15-2 df
# multiply by 2 for a two-sided test
pt(t, 13, lower.tail = FALSE)*2 # < 0.05, REJECT H0: it is unlikely that for this data
void main() {
library(cpp11)
detach("package:cpp11", unload = TRUE)
