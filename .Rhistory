y <- c(15, 14, 12, 8, 14, 7, 16, 10, 15, 12)
# normality test
# with a qqplot
par(mfrow=c(1,2))
qqnorm(x); qqline(x, col="red")
qqnorm(y); qqline(y, col="red")
# with the Shapiro-Wilk test
shapiro.test(x)
shapiro.test(y)
# normality test
# with a qqplot
par(mfrow=c(1,2))
qqnorm(x); qqline(x, col="red"); qqnorm(y); qqline(y, col="red")
# with the Shapiro-Wilk test
shapiro.test(x); shapiro.test(y)
# homogeneity of variances
var(x)/var(y)
var.test(x,y)
#t-test
t.test(x,y,
paired = FALSE,
var.equal = TRUE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = FALSE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = FALSE,
alternative = "two.sided")
x <- c(3, 0, 5, 2, 5, 5, 5, 4, 4, 5)
y <- c(2, 1, 4, 1, 4, 3, 3, 2, 3, 5)
# normality test
# with a qqplot
par(mfrow=c(1,2))
qqnorm(x); qqline(x, col="red")
qqnorm(y); qqline(y, col="red")
# with the Shapiro-Wilk test
shapiro.test(x)
shapiro.test(y)
# homogeneity of variances
var.test(x,y)
# t-test
#t-test
t.test(x,y,
paired = TRUE,
var.equal = TRUE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = TRUE,
alternative = "two.sided")
# non parametric test
wilcox.test(x,y,
paired = TRUE,
alternative = "two.sided")
rep(5/2, 5)
library(dplyr)
## chi2 - goodness of fit test
freq <- c(22, 21, 22, 27, 22, 36)
probs <- rep(1/6, 6)
chisq.test(freq, p = probs)
## chi2 - test of independence
yesbelt <- c(12813, 647, 359, 42)
nobelt <- c(65963, 4000, 2642, 303)
chisq.test(data.frame(yesbelt, nobelt))
# names(chisq.test(data.frame(yesbelt, nobelt)))
# data.frame(yesbelt, nobelt)
# rowSums(data.frame(yesbelt, nobelt))
# colSums(data.frame(yesbelt, nobelt))
# # for the first cell
# 78776*13861/sum(data.frame(yesbelt, nobelt))
# # for the whole dataframe
# n = sum(data.frame(yesbelt, nobelt))
# data.frame(yesbelt, nobelt) %>%
#   mutate(somma.riga = rowSums(data.frame(yesbelt, nobelt)),
#          e.yesbelt = somma.riga*sum(yesbelt)/n,
#          e.nobelt = somma.riga*sum(nobelt)/n)
# # compare with the values calculated by R
# chisq.test(data.frame(yesbelt, nobelt))$exp
#
# chisq.test(data.frame(yesbelt, nobelt))
## chi2 - test of homogeneity
die.fair <- sample(1:6, 200, p=c(1,1,1,1,1,1)/6, replace = TRUE)
die.bias <- sample(1:6, 200, p=c(0.5,0.5,1,1,1,2)/6, replace = TRUE)
res.fair <- table(die.fair)
res.bias <- table(die.bias)
rbind(res.fair, res.bias)
chisq.test(rbind(res.fair, res.bias))
names(chisq.test(rbind(res.fair, res.bias)))
freq <- c(53, 22, 49)
probs <- c(5/12, 3/12, 4/12)
chisq.test(freq, p = probs)
r1 <- c(3,1)
r2 <- c(1,0)
chisq.test(data.frame(r1,r2)) -> Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
r1 <- c(3,1)
r2 <- c(1,0)
chisq.test(data.frame(r1,r2)) -> Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
r1 <- c(3,1)
r2 <- c(1,0)
chisq.test(data.frame(r1,r2)) -> Xsq
Xsq$observed   # observed counts (same as M)
Xsq$expected
Xsq
birds_habitats <- data.frame(
species = c("Ruby-crowed kinglet", "White-crowned sparrow", "Lincoln's sparrow",
"Golgen-crowded sparrow", "Bushtit", "Song sparrow", "Spotted towhee",
"Bewick's wren", "Hermit thrush", "Dard-eyed junco",
"Lesser goldfinch", "Uncommon"),
Remnant = c(677, 408, 270, 300, 198, 150, 137, 106, 119, 34, 57, 457),
Restored = c(198, 260, 187, 89, 91, 50, 32, 48, 24, 39, 15, 125)
)
birds_habitats
chisq.test(data.frame(birds_habitats$Remnant, birds_habitats$Restored))
# ?cars
summary(cars)
?cars
# ?cars
summary(cars)
# ?cars
summary(cars)
cars.lm <- lm(dist ~ speed, data = cars)
names(cars.lm)
coef(cars.lm)
cars.lm
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red", lwd=2)
summary(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red", lwd=2)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red", lwd=2)
summary(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
plot(dist ~ speed, data = cars, pch = 16)
abline(cars.lm, col="red", lwd=2)
plot(dist ~ speed, data = cars, pch = 16)
abline(cars.lm, col="red", lwd=2)
summary(cars.lm)
-17.58 + 3.93 * (8) # point estimate
cars[5,] # real data
fitted(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
predict(cars.lm, newdata = data.frame(speed = c(0,6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
col="blue", pch=19)
predict(cars.lm, newdata = data.frame(speed = c(0,6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
col="blue", pch=19)
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
predict(cars.lm, newdata = data.frame(speed = c(0,6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
col="blue", pch=19)
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
carslm$residuals-carsumry$sigma
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
carslm$residuals-carsumry$sigma
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
cars.lm$residuals-carsumry$sigma
cars.lm$residuals-carsumry$sigma
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
cars.lm$residuals-carsumry$sigma
summary(cars.lm)
confint(cars.lm)
# new <- data.frame(speed = c(5,6,21))
# predict(cars.lm, newdata = new, interval = "confidence")
# predict(cars.lm, newdata = new, interval = "prediction")
library(HH)
ci.plot(cars.lm)
# or
# library(UsingR)
# simple.lm(cars$speed, cars$dist, show.ci=TRUE)
x = c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
y = c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
lm(y ~ x) # the basic values of the regression analysis
lm.result = lm(y ~ x)
summary(lm.result)
plot(x,y, xlim=c(0,80), ylim=c(150,240)) # make a plot
abline(coef(lm.result)) #lm(y ~ x)) # plot the regression line
# to see only some parts of the result
coef(lm.result)
summary(resid(lm.result))
void main() {
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# SE.b0 = s*(sqrt((1/n)+(mean(x)^2/sum((x-mean(x))^2)))) # formula for the standard error of the intercept
SE.b0 = 2.86694
b0 = coef(lm.result)[[1]]
t = (b0 - 220)/SE.b0
t
pt(t, 13, lower.tail = TRUE) # FIND THE LEFT TAIL FOR THIS VALUE OF t AND 15-2 df
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# n = length(x)
# es = resid(lm.result)
# b1 = coef(lm.result)[[2]]
# s = sqrt(sum(es^2)/(n-2)) # S^2 = SSE/(n-2), s is the residual standard error
# SE = s/sqrt(sum((x-mean(x))^2)) # SE
b1 = -0.79773
SE = 0.06996
t = (b1-(-1))/SE # (obs. value - hyp.value)/SE
t
pt(t, 13, lower.tail = FALSE) # FIND THE RIGHT TAIL FOR THIS VALUE OF t AND 15-2 df
# multiply by 2 for a two-sided test
pt(t, 13, lower.tail = FALSE)*2 # < 0.05, REJECT H0: it is unlikely that for this data
void main() {
library(cpp11)
detach("package:cpp11", unload = TRUE)
void main() {
#include <Rcpp.h>
using namespace Rcpp;
cout << "Hello World!\n";
#include <Rcpp.h>
using namespace Rcpp;
# new <- data.frame(speed = c(5,6,21))
# predict(cars.lm, newdata = new, interval = "confidence")
# predict(cars.lm, newdata = new, interval = "prediction")
library(HH)
ci.plot(cars.lm)
---
title: "How to do a SLR analysis with R"
# ?cars
summary(cars)
cars.lm <- lm(dist ~ speed, data = cars)
names(cars.lm)
coef(cars.lm)
cars.lm
plot(dist ~ speed, data = cars, pch = 16)
abline(cars.lm, col="red", lwd=2)
summary(cars.lm)
-17.58 + 3.93 * (8) # point estimate
cars[5,] # real data
fitted(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
predict(cars.lm, newdata = data.frame(speed = c(0,6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
col="blue", pch=19)
residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma
summary(cars.lm)
confint(cars.lm)
# new <- data.frame(speed = c(5,6,21))
# predict(cars.lm, newdata = new, interval = "confidence")
# predict(cars.lm, newdata = new, interval = "prediction")
library(HH)
ci.plot(cars.lm)
# or
# library(UsingR)
# simple.lm(cars$speed, cars$dist, show.ci=TRUE)
x = c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
y = c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
lm(y ~ x) # the basic values of the regression analysis
lm.result = lm(y ~ x)
summary(lm.result)
plot(x,y, xlim=c(0,80), ylim=c(150,240)) # make a plot
abline(coef(lm.result)) #lm(y ~ x)) # plot the regression line
# to see only some parts of the result
coef(lm.result)
summary(resid(lm.result))
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# n = length(x)
# es = resid(lm.result)
# b1 = coef(lm.result)[[2]]
# s = sqrt(sum(es^2)/(n-2)) # S^2 = SSE/(n-2), s is the residual standard error
# SE = s/sqrt(sum((x-mean(x))^2)) # SE
b1 = -0.79773
SE = 0.06996
t = (b1-(-1))/SE # (obs. value - hyp.value)/SE
t
pt(t, 13, lower.tail = FALSE) # FIND THE RIGHT TAIL FOR THIS VALUE OF t AND 15-2 df
# multiply by 2 for a two-sided test
pt(t, 13, lower.tail = FALSE)*2 # < 0.05, REJECT H0: it is unlikely that for this data
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# SE.b0 = s*(sqrt((1/n)+(mean(x)^2/sum((x-mean(x))^2)))) # formula for the standard error of the intercept
SE.b0 = 2.86694
b0 = coef(lm.result)[[1]]
t = (b0 - 220)/SE.b0
t
pt(t, 13, lower.tail = TRUE) # FIND THE LEFT TAIL FOR THIS VALUE OF t AND 15-2 df
growth = c(12, 10, 8, 11, 6, 7, 2, 3.5, 3.5)
tannin = 0:8
t = (-1.1583 - (-1.5))/0.2236
t
pt(t, 7, lower.tail = FALSE)
data(cars)
cars.lm <- lm(dist ~ speed, data = cars)
coef(cars.lm)
names(cars.lm)
residuals(cars.lm)
par(mfrow=c(1,3))
hist(residuals(cars.lm))
boxplot(residuals(cars.lm))
qqnorm(residuals(cars.lm))
qqline(residuals(cars.lm))
par(mfrow=c(1,1))
plot(cars.lm)
# install.packages("lmtest")
library(lmtest)
dwtest(cars.lm, alternative = "two.sided")
cars[23,]
dim(cars)
cars[-23,]
dim(cars[-23,])
shapiro.test(residuals(cars.lm))
library(lmtest)
bptest(cars.lm)
data("trees")
head(trees)
hist(trees$Volume)
hist(trees$Volume^2)
library(MASS)
?boxcox
bctrans <- boxcox(Volume ~ Height + Girth, data = trees)
bctrans <- boxcox(Volume ~ Height + Girth, data = trees,
lambda = seq(0, 0.5, length = 10))
bctrans
which(bctrans$y==max(bctrans$y))
bctrans$x[62]
hist(trees$Volume)
hist(trees$Volume^0.31)
max(bctrans$y)
library(dplyr)
risultati.bc <- data.frame(bctrans$y, bctrans$x)
head(risultati.bc)
arrange(risultati.bc, bctrans.y)
which(bctrans$y==max(bctrans$y)) %>%
bctrans$x[]
which(bctrans$y==max(bctrans$y))
bctrans$x[62]
data("trees")
head(trees)
hist(trees$Volume)
hist(trees$Volume^2)
library(MASS)
?boxcox
bctrans <- boxcox(Volume ~ Height + Girth, data = trees)
bctrans <- boxcox(Volume ~ Height + Girth, data = trees,
lambda = seq(0, 0.5, length = 10))
bctrans
which(bctrans$y==max(bctrans$y))
bctrans$x[62]
hist(trees$Volume)
hist(trees$Volume^0.31)
max(bctrans$y)
library(dplyr)
risultati.bc <- data.frame(bctrans$y, bctrans$x)
head(risultati.bc)
arrange(risultati.bc, bctrans.y)
# load the data
data(trees) # this is a dataframe
class(trees)
head(trees)
str(trees)
# histograms
hist(trees$Girth)
hist(trees$Height)
hist(trees$Volume)
# boxplots
boxplot(trees$Girth)
boxplot(trees$Height)
boxplot(trees$Volume)
# normality test
shapiro.test(trees$Girth)
qqnorm(trees$Girth); qqline(trees$Girth)
# normality test
shapiro.test(trees$Height)
qqnorm(trees$Height); qqline(trees$Height)
# normality test
shapiro.test(trees$Volume)
qqnorm(trees$Volume); qqline(trees$Volume)
data("trees")
mod <- lm(Volume ~ Girth + Height, data=trees)
summary(mod)
# the mean increase in volume when there is a one-unit increase in girth is 4.7
# Volume.tr <- trees$Volume^0.30
# mod.tr <- lm(Volume.tr ~ Girth + Height, data=trees)
# summary(mod.tr)
plot(Volume ~ Girth, data=trees)
mod.q <- lm(Volume ~ Girth + I(Girth^2) +
Height + I(Height^2),
data=trees)
summary(mod.q)
# alternative code:
mod.q <- lm(Volume ~ poly(Girth, 2, raw=TRUE) + poly(Height, 2, raw=TRUE), data=trees)
summary(mod.q)
plot(Volume ~ Girth, data=trees)
mod.q <- lm(Volume ~ Girth + I(Girth^2) + Height , data=trees)
summary(mod.q)
mod.int <- lm(Volume ~ Girth * Height, data=trees)
summary(mod.int)
mod <- lm(Volume ~ Girth * Height, data=trees)
plot(mod)
plot(trees$Volume ~ trees$Girth)
plot(trees$Volume ~ trees$Height)
library(sjPlot)
data("trees")
mod <- lm(Volume ~ Girth + Height, data=trees)
summary(mod)
# the mean increase in volume when there is a one-unit increase in girth is 4.7
# Volume.tr <- trees$Volume^0.30
# mod.tr <- lm(Volume.tr ~ Girth + Height, data=trees)
# summary(mod.tr)
library(sjPlot)
# plot_model(mod)
plot_model(mod, type = "eff", terms = "Girth")
plot_model(mod, type = "eff", terms = "Girth")
plot_model(mod, type = "eff", terms = "Girth")
plot_model(mod, type = "eff", terms = "Girth")
library(sjPlot)
# plot_model(mod)
plot_model(mod, type = "eff", terms = "Girth")
plot_model(mod.int, type = "eff", terms = "Height")
plot_model(mod.int, type = "int", terms = c("Height","Girth")) # # type = "int" automatically selects groups for continuous moderator variables
library(jtools)
effect_plot(mod.int, pred = Girth, interval = TRUE)
plot_model(mod.int, type = "pred", terms = c("Girth", "Height [60,80]"))
# switch moderator
plot_model(mod.int, type = "pred", terms = c("Height", "Girth"))
plot_model(mod.int, type = "est", terms = c("Height", "Grith"))
plot_model(mod.int, type = "est", terms = c("Height", "Grith"))
plot_model(mod.int, type = "est")
knitr::opts_chunk$set(echo = TRUE)
trees$Tall <- cut(trees$Height, breaks = c(-Inf, 76, Inf),
labels = c("no", "yes"))
trees$Tall[1:5]
class(trees$Tall)
treesdummy.lm <- lm(Volume ~ Girth + Tall, data = trees)
summary(treesdummy.lm)
# # we split the trees data into
# # two pieces, with groups determined by the Tall variable:
# treesTall <- split(trees, trees$Tall)
# # we add the Fitted values to each
# # piece via predict
# treesTall[["yes"]]$Fit <- predict(treesdummy.lm, treesTall[["yes"]])
# treesTall[["no"]]$Fit <- predict(treesdummy.lm, treesTall[["no"]])
# # we set up a plot for the variables Volume versus Girth
# # but we do not
# # plot anything yet (type = n) because we want to use different symbols for the two groups
# plot(Volume ~ Girth, data = trees, type = "n")
# # we add points to the plot for the Tall = yes trees and use an open circle for a plot character
# # (pch = 1), followed by points for the Tall = no trees with a triangle character (pch = 2).
# points(Volume ~ Girth, data = treesTall[["yes"]], pch = 1)
# points(Volume ~ Girth, data = treesTall[["no"]], pch = 2)
# # we add regression lines to the plot, one for each group
# lines(Fit ~ Girth, data = treesTall[["yes"]])
# lines(Fit ~ Girth, data = treesTall[["no"]])
plot(Volume ~ Girth, data = trees)
abline(a = -34, b = 4.69, col="red") # alberi bassi
abline(a = -30, b = 4.69, col="blue") # alberi alti
treesdummy.lm <- lm(Volume ~ Tall, data = trees)
summary(treesdummy.lm)
head(trees)
tall.trees <- subset(trees, trees$Tall=="yes")
small.trees <- subset(trees, trees$Tall=="no")
t.test(tall.trees$Volume, small.trees$Volume)
scores.graderA = c(4,3,4,5,2,3,4,5)
scores.graderB = c(4,4,5,5,4,5,4,4)
scores.graderC = c(3,4,2,4,5,5,4,4)
scores <- c(scores.graderA,scores.graderB,scores.graderC)
graders <- c(rep("A",8), rep("B",8), rep("C",8))
scores.graders <- data.frame(graders,scores)
boxplot(scores ~ graders, data=scores.graders)
mod <- lm(scores ~ graders, data=scores.graders)
summary(mod)
summary(aov(scores ~ graders, data=scores.graders))
# oppure
anova.prova <- aov(scores ~ graders, data=scores.graders)
summary(anova.prova)
anova(lm(scores ~ graders, data=scores.graders))
