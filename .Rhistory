# cor.matrix
p.mat <- (abs(cor.matrix) > 0.7)
# p.mat
ggcorrplot(cor.matrix,
hc.order = TRUE, method = "circle",
type = "lower", p.mat = p.mat
)
# Build the model
model1 <- lm(Pollution ~ ., data = pollution)
summary(model1)
vif(model1)
model1 <- lm(Pollution ~ Temp + Industry + Wind + Rain + Wet.days, data = pollution)
summary(model1)
dati.mod <- read.csv("data/dati.mod.csv")
head(dati.mod)
varexpl <- dati.mod[,8:15]
head(varexpl)
hist(dati.mod$CattSiNo)
table(dati.mod$CattSiNo)
summary(varexpl)
with(dati.mod,
boxplot(Incl ~ CattSiNo, main="Slope"))
with(dati.mod,
boxplot(Alt ~ CattSiNo, main="Altitude"))
with(dati.mod,
boxplot(copveg ~ CattSiNo, main="Veg cover"))
with(dati.mod,
boxplot(NSpecie ~ CattSiNo, main="N species"))
with(dati.mod,
boxplot(media.rocce ~ CattSiNo, main="Mean diam"))
with(dati.mod,
boxplot(mediana.rocce ~ CattSiNo, main="Median diam"))
with(dati.mod,
boxplot(sd.rocce ~ CattSiNo, main="SD diam"))
with(dati.mod,
boxplot(Esp.tr ~ CattSiNo, main="Aspect"))
par(mfrow=c(1,2))
dotchart(dati.mod$sd.rocce,
ylab = "Order of observations",
xlab = "SD diam", main = "")
dotchart(dati.mod$sd.rocce,
groups = dati.mod$CattSiNo,
ylab = "CattSiNo",
xlab = "SD diam", main = "")
par(mfrow=c(1,1))
# eliminazione dell'outlier
subset(dati.mod, dati.mod$sd.rocce > 200)
dati.mod.new <- subset(dati.mod, dati.mod$sd.rocce < 200)
# trasformazione della variabile x
boxplot(dati.mod$sd.rocce)
boxplot(log(dati.mod$sd.rocce))
dati.mod.new2 <- cbind(dati.mod, logSD = log(dati.mod$sd.rocce))
dati.mod.new2 <- data.frame(dati.mod, logSD = log(dati.mod$sd.rocce))
head(dati.mod.new2)
cor(varexpl)
fm1 <- glm(CattSiNo ~ Incl +  # BEYOND OPTIMAL MODEL
Alt +
copveg +
# NSpecie +
# media.rocce +
mediana.rocce +
sd.rocce +
Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1)
hist(dati.mod.new$Ncatture)
fm1.pois <- glm(Ncatture ~ Incl +
Alt +
copveg +
# NSpecie +
# media.rocce +
mediana.rocce +
sd.rocce +
Esp.tr,
family=poisson, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1.pois)
# UP TO YOU!!!
fm1.noexp <- glm(CattSiNo ~ Incl +
Alt,
#copveg,
# NSpecie +
# media.rocce +
#mediana.rocce,
# sd.rocce, #
# Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1.noexp)
# UP TO YOU!!!
fm1.noexp <- glm(CattSiNo ~ Incl +
Alt,
#copveg +
# NSpecie +
# media.rocce +
#mediana.rocce,
# sd.rocce, #
# Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm1.noexp)
fm2 <- glm(CattSiNo ~ #Incl +
Alt +
# copveg +
# NSpecie +
media.rocce, #+
# mediana.rocce +
# sd.rocce +
# Esp.tr,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
par(mfrow=c(1,2))
termplot(fm2, se=TRUE, col.term = "red", col.se = "grey",
ylabs = rep("odds ratio",2),
main = c("Alt","Mean diam"), cex.main = 0.7)
par(mfrow=c(1,1))
logit2prob <- function(logit){
odds <- exp(logit)
prob <- odds / (1 + odds)
return(prob)
}
predict.alt <- predict(fm2,
data.frame(Alt=dati.mod.new$Alt,
media.rocce=mean(dati.mod.new$media.rocce)),
type = "response")
tab <- data.frame(Alt=dati.mod.new$Alt,
prob = predict.alt)
# tab
library(dplyr)
arrange(tab, Alt)
plot(arrange(tab, Alt))
library(sjPlot)
plot_model(fm2, type = "eff", terms = c("Alt"))
plot_model(fm2)
fm2 <- glm(CattSiNo ~ Alt + I(Alt^2),
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
# ok:
fm2 <- glm(CattSiNo ~ Alt,
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
# no:
fm2 <- glm(CattSiNo ~ I(Alt^2),
family=binomial, data=dati.mod.new)
#, offset=Noccasioni)
summary(fm2)
knitr::opts_chunk$set(echo = TRUE)
res.prcomp <- prcomp(iris[,-5])
summary(res.prcomp)
# compare to:
res.prcomp <- prcomp(iris[,-5], center=TRUE, scale=TRUE)
summary(res.prcomp)
plot(res.prcomp)
biplot(res.prcomp)
res.prcomp$rotation
biplot(res.prcomp, choices=3:4)
res.princomp <- princomp(iris[,-5])
summary(res.princomp)
# compare to:
res.princomp <- princomp(iris[,-5], cor=TRUE)
summary(res.princomp)
loadings(res.princomp) # Small loadings are conventionally not printed (replaced by spaces), to draw the eye to the pattern of the larger loadings.
# A matrix of loadings, one column for each factor. The factors are ordered in decreasing order of sums of squares of loadings, and given the sign that will make the sum of the loadings positive.
# The signs of the loadings vectors are arbitrary
# Calling loadings() on your object returns a summary where the SS are always equal to 1, hence the % variance is just the SS loadings divided by the number of variables. It makes sense only when using Factor Analysis - don't look at this table for PCA!!
plot(res.princomp)
biplot(res.princomp)
biplot(res.prcomp)
biplot(res.prcomp)
res.prcomp$rotation
res.prcomp <- prcomp(iris[,-5])
summary(res.prcomp)
# compare to:
res.prcomp <- prcomp(iris[,-5], center=TRUE, scale=TRUE)
summary(res.prcomp)
plot(res.prcomp)
biplot(res.prcomp)
res.prcomp$rotation
biplot(res.prcomp, choices=3:4)
res.princomp <- princomp(iris[,-5])
summary(res.princomp)
# compare to:
res.princomp <- princomp(iris[,-5], cor=TRUE)
summary(res.princomp)
loadings(res.princomp) # Small loadings are conventionally not printed (replaced by spaces), to draw the eye to the pattern of the larger loadings.
# A matrix of loadings, one column for each factor. The factors are ordered in decreasing order of sums of squares of loadings, and given the sign that will make the sum of the loadings positive.
# The signs of the loadings vectors are arbitrary
# Calling loadings() on your object returns a summary where the SS are always equal to 1, hence the % variance is just the SS loadings divided by the number of variables. It makes sense only when using Factor Analysis - don't look at this table for PCA!!
plot(res.princomp)
biplot(res.princomp)
library(FactoMineR)
library(factoextra)
res.pca <- PCA(iris[, -5],  graph = FALSE)
res.pca
get_eig(res.pca) # Extract eigenvalues/variances
sqrt(get_eig(res.pca)[,1]) # compare to summary(res.prcomp)
# Visualize eigenvalues/variances
fviz_eig(res.pca, addlabels=TRUE, hjust = -0.3)+
theme_minimal()
# export all results in csv file
write.infile(res.pca,file="my_FactoMineR_results.csv")
varimax(res.princomp$loadings[,1:2])
# Varimax is so called because it maximizes the sum of the variances of the squared loadings (squared correlations between variables and factors).
# After an orthogonal rotation (such as varimax), the "rotated-principal" axes are not orthogonal, and orthogonal projections on them do not make sense. So one should rather drop this whole axes/projections point of view. It would be weird to still call it PCA (which is all about projections with maximal variance etc.).
# Extract the results for variables
var <- get_pca_var(res.pca)
var
# Contribution of variables
var$contrib
colSums(var$contrib)
# correlation of variables with the first 4 PC
res.pca$var$cor[, 1:4]
# axis characterization
dimdesc(res.pca)
# Graph of variables: default plot
fviz_pca_var(res.pca, col.var = "steelblue")
# Itâ€™s possible to control variable colors using their contributions to the principal axes:
# Control variable colors using their contributions
# Use gradient color
fviz_pca_var(res.pca, col.var="contrib")+
scale_color_gradient2(low="white", mid="blue",
high="red", midpoint = 25) +
theme_minimal()
# Variable contributions on axis 1
fviz_contrib(res.pca, choice="var", axes = 1 )+
labs(title = "Contributions to Dim 1")
# Variable contributions on axis 2
fviz_contrib(res.pca, choice="var", axes = 2 )+
labs(title = "Contributions to Dim 2")
# Variable contributions on axes 1 + 2
fviz_contrib(res.pca, choice="var", axes = 1:2)+
labs(title = "Contributions to Dim 1+2")
# Extract the results for individuals
ind <- get_pca_ind(res.pca)
ind
# Coordinates of individuals
head(ind$coord)
# compare to:
head(res.prcomp$x)
# Graph of individuals
# 1. Use repel = TRUE to avoid overplotting
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
print(p)
# Change group colors manually
# Read more: http://www.sthda.com/english/wiki/ggplot2-colors
p + scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
theme_minimal()
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Extract the results for individuals
ind <- get_pca_ind(res.pca)
ind
# Coordinates of individuals
head(ind$coord)
# compare to:
head(res.prcomp$x)
# Graph of individuals
# 1. Use repel = TRUE to avoid overplotting
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
print(p)
# Change group colors manually
# Read more: http://www.sthda.com/english/wiki/ggplot2-colors
p + scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
theme_minimal()
colorvar<-col="red"
colorvar->col="red"
colorvar<-col="red"
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
# Extract the results for individuals
ind <- get_pca_ind(res.pca)
ind
# Coordinates of individuals
head(ind$coord)
# compare to:
head(res.prcomp$x)
# Graph of individuals
# 1. Use repel = TRUE to avoid overplotting
fviz_pca_ind(res.pca, repel = TRUE)+
theme_minimal()
# Color by groups: habillage=iris$Species
# Show points only: geom = "point"
p <- fviz_pca_ind(res.pca, geom = "point",
habillage=iris$Species, addEllipses=TRUE,
ellipse.level= 0.95)+ theme_minimal()
print(p)
# Change group colors manually
# Read more: http://www.sthda.com/english/wiki/ggplot2-colors
p + scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
theme_minimal()
# Biplot of individuals and variables
# ++++++++++++++++++++++++++
# Only variables are labelled
fviz_pca_biplot(res.pca,  label="var", habillage=iris$Species,
addEllipses=TRUE, ellipse.level=0.95) +
theme_minimal()
# # df <- USArrests[,1:2]
# df_birds <- read_xlsx("data/short.xlsx", col_types = c("text","numeric","numeric"))
# df <- na.omit(df_birds)
# df <- scale(df[,2:3])
# head(df)
european_birds <- read_delim("data/european_birds.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
fviz_nbclust(df, kmeans, method = "wss")
---
title: "K-means clustering - step by step"
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(FactoMineR)
library(readxl)
library(readr)
# # df <- USArrests[,1:2]
# df_birds <- read_xlsx("data/short.xlsx", col_types = c("text","numeric","numeric"))
# df <- na.omit(df_birds)
# df <- scale(df[,2:3])
# head(df)
european_birds <- read_delim("data/european_birds.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
european_birds %>%
dplyr::select(Species,
LengthU_MEAN,
# WingU_MEAN,
Clutch_MEAN,
`Life span`) -> df_birds
df_birds <- na.omit(df_birds)
df <- scale(df_birds[,2:dim(df_birds)[2]])
head(df)
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
fviz_nbclust(df, kmeans, method = "wss")
fviz_nbclust(df, kmeans, method = "silhouette")
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 3, nstart = 25)
print(final)
fviz_cluster(final, data = df,
geom = c("point"),
ellipse.type = "euclid")
final$cluster
df_birds %>%
mutate(cluster_id = final$cluster) -> df_birds_cluster
write.csv(df_birds_cluster, file = "output/df_birds_cluster.csv")
writexl::write_xlsx(df_birds_cluster, "output/df_birds_cluster.xlsx")
head(df_birds_cluster)
sample(x = 1:6, size = 5, replace = FALSE)
sample(x = 1:6, size = 20, replace = TRUE)
sample(x = 1:6, size = 20, replace = TRUE) # you get different numbers
sample(x = 1:6, size = 20, replace = TRUE) # again
RollDie <- function(n) sample(1:6, n, replace = TRUE)
d1 <- RollDie(n = 50)
sum(d1 == 6)
sum(d1 == 6)/50
hist(d1)
hist(d1, probability = TRUE, breaks = seq(0.5,6.5,1))
sims <- vector("list", 500)
probs <- vector("numeric", 500)
for (n in 1:500) {
sims[[n]] <- RollDie(n)
probs[n] <- sum(sims[[n]] == 6)/n
}
plot(probs)
abline(h=1/6)
sample(x=c("testa","croce"), size = 5, replace = TRUE)
sample(1:6, size = 1, replace = TRUE)
sample(1:6, size = 1, replace = TRUE)
roll1 = NULL  #This initializes our variable - i.e. it creates a spot in memory for it. We need to do this for any vector, table, matrix, dataframe, but not for single numbers
roll2 = NULL
for (i in 1:100) {
roll1[i] = RollDie(1)
roll2[i] = RollDie(1)
}
# We can ask how many doubles we came up with:
sum(roll1 == roll2)
# relative frequency
sum(roll1 == roll2)/100
roll1 <- vector("list", 500)
roll2 <- vector("list", 500)
probs <- vector("numeric", 500)
for (n in 1:500) {
roll1[[n]] <- RollDie(n)
roll2[[n]] <- RollDie(n)
probs[n] <- sum(roll1[[n]] == roll2[[n]])/n
}
plot(probs)
abline(h=1/6)
n = 500
roll1 = NULL  #This initializes our variable - i.e. it creates a spot in memory for it. We need to do this for any vector, table, matrix, dataframe, but not for single numbers
roll2 = NULL
for (i in 1:n) {
roll1[i] = RollDie(1)
roll2[i] = RollDie(1)
}
hist((roll1 + roll2), density = 100, breaks = 1:12, prob = T)
barplot(table(roll1 + roll2), main = "2 Dice Sum, 100 Rolls")  #this works better for this case
rolls <- roll1 + roll2
sum(rolls == 7)
sum(rolls == 7)/n
n = 1500
roll1 = NULL  #This initializes our variable - i.e. it creates a spot in memory for it. We need to do this for any vector, table, matrix, dataframe, but not for single numbers
roll2 = NULL
for (i in 1:n) {
roll1[i] = RollDie(1)
roll2[i] = RollDie(1)
}
# We can ask how many times this happen:
sum(roll1 == 6 & roll2 == 6)
# the relative frequency is:
sum(roll1 == 6 & roll2 == 6)/n
(1/6)*(1/6)
0.02777778+0.02777778+0.02777778+0.02777778+0.02777778+0.02777778
# or
0.02777778*6
y <- read.csv("data/captures.csv",sep=";")
y <- na.omit(y[,c("age", "sex")]); y
library(dplyr)
age.marginal.df <-
y %>%
group_by(age) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
age.marginal.df
sex.marginal.df <-
y %>%
group_by(sex) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
sex.marginal.df
joint.df <-
y %>%
group_by(age, sex) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(prop = n/sum(n))
joint.df
joint.prob <-
joint.df %>%
filter(age == "A", sex == "F") %>%
.$prop
joint.prob
marg.prob <-
age.marginal.df %>%
filter(age == "A") %>%
.$prop
marg.prob
cond.prob <- joint.prob/marg.prob
cond.prob
library(Rmisc)
library(Hmisc)
library(ggplot2)
library(boot)
dati <- read.csv("data/captures.csv", sep=";")
CI(dati$weight_g, ci=0.95)
CI(na.omit(dati$weight_g), ci=0.95)
dati2 <- na.omit(dati)
group.CI(weight_g ~ sex,
data= dati2,
ci=0.95)
intervalli <- group.CI(weight_g ~ sex,
data= dati2,
ci=0.95)
intervalli
ggplot(intervalli, aes(x=sex, y=weight_g.mean, colour=sex)) +
geom_errorbar(aes(ymin=weight_g.lower, ymax=weight_g.upper), width=.1) +
geom_line() +
geom_point()
as.data.frame(CI(na.omit(dati$weight_g), ci=0.95)) -> CImean
CImean
variabile <- dati2$footlength_mm # o qualsiasi altra variabile numerica
# script 'generalizzato'
intervalli <- group.CI(variabile ~ sex,
data= dati2,
ci=0.95)
intervalli
ggplot(intervalli, aes(x=sex, y=variabile.mean, colour=sex)) +
geom_errorbar(aes(ymin=variabile.lower, ymax=variabile.upper), width=.1) +
geom_line() +
geom_point()
boot.data <- boot(dati2$weight_g,
function(x,i) mean(x[i]),
R=10000)
boot.ci(boot.data,
conf = 0.95)
